default-model: llama3
apis:
  openai:
    # base-url: https://api.openai.com/v1
    base-url: http://localhost:11434/v1
    api-key: ollama
    api-key-env: OPENAI_API_KEY
    models:
      llama3:
        max-input-chars: 24500
  groq:
    base-url: https://api.groq.com/openai/v1
    api-key:
    api-key-env: GROQ_API_KEY
    models:
      llama3-70b-8192:
        aliases: ["groq-llama3"]
        max-input-chars: 24500
# Text to append when using the -f flag.
format-text:
  markdown: "Format the response as markdown without enclosing backticks."
  json: "Format the response as json without enclosing backticks."
# List of predefined system messages that can be used as roles.
roles:
  "default": []
# System role to use.
role: "default"
# Ask for the response to be formatted as markdown unless otherwise set.
format: true
# Render output as raw text when connected to a TTY.
raw: false
# Quiet mode (hide the spinner while loading and stderr messages for success).
quiet: false
# Temperature (randomness) of results, from 0.0 to 2.0.
temp: 0.1
# TopP, an alternative to temperature that narrows response, from 0.0 to 1.0.
topp: 0.5
# Turn off the client-side limit on the size of the input into the model.
no-limit: true
# Wrap formatted output at specific width (default is 80)
word-wrap: 100
# Include the prompt from the arguments in the response.
include-prompt-args: false
# Include the prompt from the arguments and stdin, truncate stdin to specified number of lines.
include-prompt: 0
# Maximum number of times to retry API calls.
max-retries: 5
# Your desired level of fanciness.
fanciness: 10
# Text to show while generating.
status-text: Thinking...
# Default character limit on input to model.
max-input-chars: 12250
